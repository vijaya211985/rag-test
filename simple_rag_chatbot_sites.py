# -*- coding: utf-8 -*-
"""Simple_RAG_Chatbot_Sites.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yKDCxzntUB6AEjSRl-MPsqXwkBjs3xaX

# RAG

Data is a website



```
# def generate_response(context, question):
    prompt = f"Context: {context} Answer the question clearly and concisely. Question: {question}"
    response = generator(prompt, max_length=250, num_return_sequences=1)
    return response[0]["generated_text"]
```
"""

!pip install chromadb

from transformers import AutoTokenizer, AutoModel, pipeline
from chromadb import Client
from bs4 import BeautifulSoup
import requests

# Initialize ChromaDB Client
client = Client()

from chromadb.errors import InvalidCollectionException  # Import the correct exception

# Try to get or create the collection
try:
    collection = client.get_collection(name="chatbot_data")
except InvalidCollectionException:  # Handle the specific exception
    collection = client.create_collection(name="chatbot_data")

# Step 1: Fetch Website Content and Preprocess Text
def fetch_website_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    # Extract text from the webpage, focusing on paragraphs
    paragraphs = soup.find_all("p")
    content = " ".join([p.get_text() for p in paragraphs])
    return preprocess_text(content)

def preprocess_text(text):
    return text.lower().strip()

# Fetch and preprocess website content
website_url = "https://www.nature.com/articles/s41598-024-79715-2" # "https://example.com"  # Replace with the target website URL
data = [fetch_website_content(website_url)]
preprocessed_data = [preprocess_text(d) for d in data]

# Step 2: Generate Embeddings
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def generate_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    # Use mean pooling for sentence embeddings
    return outputs.last_hidden_state.mean(dim=1).detach().numpy().tolist()[0]

# Validate embeddings
def validate_embedding(embedding):
    if not all(isinstance(value, (int, float)) for value in embedding):
        raise ValueError("Embedding contains non-numeric values.")

# Generate and store embeddings
embeddings = [generate_embedding(text) for text in preprocessed_data]

# Validate embeddings before adding to the collection
for embedding in embeddings:
    validate_embedding(embedding)

collection.add(
    ids=["1"],  # Use unique IDs for documents
    embeddings=embeddings,
    metadatas=[{"text": data[0]}]
)
print("Website data stored successfully!")

# Step 4: Retrieval Mechanism
def retrieve_context(user_input):
    query_embedding = generate_embedding(user_input)
    results = collection.query(query_embeddings=[query_embedding], n_results=1)
    if results["documents"]:
        return results["documents"][0]
    return None

# Step 5: Generative Model for Responses
generator = pipeline("text-generation", model="gpt2")

def generate_response(context, question):
    prompt = f"Context: {context} Answer the question clearly and concisely. Question: {question}"
    response = generator(prompt, max_length=250, num_return_sequences=1)
    return response[0]["generated_text"]

# Step 6: Combining Retrieval and Generation
def chatbot_response(user_input):
    context = retrieve_context(user_input)
    if not context:
        return "I'm sorry, I couldn't find relevant information."
    return generate_response(context, user_input)

# Testing the Chatbot
print(chatbot_response("What is the Basic characteristics of participants from this website?"))

# Testing the Chatbot
print(chatbot_response("What is the Basic characteristics of participants from this website?"))